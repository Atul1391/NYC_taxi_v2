{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785c9dae-22ba-4e20-888d-d0252461d346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ff420f-6707-4bac-ba90-19cdd9ee680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cbac49-9953-4575-b84e-ef85c2488973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download raw data from 2023 to 2024\n",
      "File already exists for 2023-01.\n",
      "Loading data for 2023-01...\n",
      "Total records: 3,066,766\n",
      "Valid records: 2,993,140\n",
      "Records dropped: 73,626 (2.40%)\n",
      "Successfully processed data for 2023-01.\n",
      "File already exists for 2023-02.\n",
      "Loading data for 2023-02...\n",
      "Total records: 2,913,955\n",
      "Valid records: 2,845,058\n",
      "Records dropped: 68,897 (2.36%)\n",
      "Successfully processed data for 2023-02.\n",
      "File already exists for 2023-03.\n",
      "Loading data for 2023-03...\n",
      "Total records: 3,403,766\n",
      "Valid records: 3,331,705\n",
      "Records dropped: 72,061 (2.12%)\n",
      "Successfully processed data for 2023-03.\n",
      "File already exists for 2023-04.\n",
      "Loading data for 2023-04...\n",
      "Total records: 3,288,250\n",
      "Valid records: 3,214,922\n",
      "Records dropped: 73,328 (2.23%)\n",
      "Successfully processed data for 2023-04.\n",
      "File already exists for 2023-05.\n",
      "Loading data for 2023-05...\n",
      "Total records: 3,513,649\n",
      "Valid records: 3,435,875\n",
      "Records dropped: 77,774 (2.21%)\n",
      "Successfully processed data for 2023-05.\n",
      "File already exists for 2023-06.\n",
      "Loading data for 2023-06...\n",
      "Total records: 3,307,234\n",
      "Valid records: 3,233,969\n",
      "Records dropped: 73,265 (2.22%)\n",
      "Successfully processed data for 2023-06.\n",
      "File already exists for 2023-07.\n",
      "Loading data for 2023-07...\n",
      "Total records: 2,907,108\n",
      "Valid records: 2,838,637\n",
      "Records dropped: 68,471 (2.36%)\n",
      "Successfully processed data for 2023-07.\n",
      "File already exists for 2023-08.\n",
      "Loading data for 2023-08...\n",
      "Total records: 2,824,209\n",
      "Valid records: 2,758,739\n",
      "Records dropped: 65,470 (2.32%)\n",
      "Successfully processed data for 2023-08.\n",
      "File already exists for 2023-09.\n",
      "Loading data for 2023-09...\n",
      "Total records: 2,846,722\n",
      "Valid records: 2,782,920\n",
      "Records dropped: 63,802 (2.24%)\n",
      "Successfully processed data for 2023-09.\n",
      "File already exists for 2023-10.\n",
      "Loading data for 2023-10...\n",
      "Total records: 3,522,285\n",
      "Valid records: 3,446,406\n",
      "Records dropped: 75,879 (2.15%)\n",
      "Successfully processed data for 2023-10.\n",
      "File already exists for 2023-11.\n",
      "Loading data for 2023-11...\n",
      "Total records: 3,339,715\n",
      "Valid records: 3,267,940\n",
      "Records dropped: 71,775 (2.15%)\n",
      "Successfully processed data for 2023-11.\n",
      "File already exists for 2023-12.\n",
      "Loading data for 2023-12...\n",
      "Total records: 3,376,567\n",
      "Valid records: 3,313,957\n",
      "Records dropped: 62,610 (1.85%)\n",
      "Successfully processed data for 2023-12.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "Data loading complete.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "\n",
    "from_year = 2023\n",
    "# to_year = datetime.now().year\n",
    "to_year = 2024\n",
    "print(f\"Download raw data from {from_year} to {to_year}\")\n",
    "\n",
    "rides = pd.DataFrame()\n",
    "chunks = []\n",
    "for year in range(from_year, to_year+1):\n",
    "\n",
    "    rides_one_year = load_and_process_taxi_data(year)\n",
    "\n",
    "    chunks.append(rides_one_year)\n",
    "    break\n",
    "\n",
    "# Concatenate all chunks at the end\n",
    "rides = pd.concat(chunks, ignore_index=True)\n",
    "print(\"Data loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745492c9-9912-4834-9f70-2b72fe655c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:32:10</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:55:08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:25:04</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:03:48</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:10:29</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463263</th>\n",
       "      <td>2023-12-31 23:04:34</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463264</th>\n",
       "      <td>2023-12-31 23:08:15</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463265</th>\n",
       "      <td>2023-12-31 23:16:15</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463266</th>\n",
       "      <td>2023-12-31 23:21:58</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463267</th>\n",
       "      <td>2023-12-31 23:10:47</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37463268 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime  pickup_location_id\n",
       "0        2023-01-01 00:32:10                 161\n",
       "1        2023-01-01 00:55:08                  43\n",
       "2        2023-01-01 00:25:04                  48\n",
       "3        2023-01-01 00:03:48                 138\n",
       "4        2023-01-01 00:10:29                 107\n",
       "...                      ...                 ...\n",
       "37463263 2023-12-31 23:04:34                 233\n",
       "37463264 2023-12-31 23:08:15                  48\n",
       "37463265 2023-12-31 23:16:15                 196\n",
       "37463266 2023-12-31 23:21:58                 140\n",
       "37463267 2023-12-31 23:10:47                 237\n",
       "\n",
       "[37463268 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476b253b-c445-48f4-ae16-e9256ebb4903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37463268, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c497472c-3abb-47ed-b7c0-549c0eeb5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6373351b-3346-4aef-9133-1539319466d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2277600, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf917ec-78a1-4faa-9849-b2bbf0ef9066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int16         \n",
      " 2   rides               int16         \n",
      "dtypes: datetime64[ns](1), int16(2)\n",
      "memory usage: 26.1 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ae9fe1-ec97-46c3-a9ba-61a830df679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277595</th>\n",
       "      <td>2023-12-31 19:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277596</th>\n",
       "      <td>2023-12-31 20:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277597</th>\n",
       "      <td>2023-12-31 21:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277598</th>\n",
       "      <td>2023-12-31 22:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277599</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pickup_hour  pickup_location_id  rides\n",
       "0       2023-01-01 00:00:00                   2      0\n",
       "1       2023-01-01 01:00:00                   2      0\n",
       "2       2023-01-01 02:00:00                   2      0\n",
       "3       2023-01-01 03:00:00                   2      0\n",
       "4       2023-01-01 04:00:00                   2      0\n",
       "...                     ...                 ...    ...\n",
       "2277595 2023-12-31 19:00:00                 263    188\n",
       "2277596 2023-12-31 20:00:00                 263    198\n",
       "2277597 2023-12-31 21:00:00                 263    232\n",
       "2277598 2023-12-31 22:00:00                 263    160\n",
       "2277599 2023-12-31 23:00:00                 263     95\n",
       "\n",
       "[2277600 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb329987-abf4-4924-b763-d83dd670ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hopsworks==4.1.5 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (4.1.5)\n",
      "Requirement already satisfied: pyhumps==1.6.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2.32.3)\n",
      "Requirement already satisfied: furl in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2.1.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.36.12)\n",
      "Requirement already satisfied: pandas<2.2.0 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2.1.4)\n",
      "Requirement already satisfied: pyjks in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (20.0.0)\n",
      "Requirement already satisfied: mock in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (5.1.0)\n",
      "Requirement already satisfied: avro==1.11.3 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2.0.29)\n",
      "Requirement already satisfied: PyMySQL[rsa] in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.1.1)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (5.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2025.2.0)\n",
      "Requirement already satisfied: retrying in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.3.4)\n",
      "Requirement already satisfied: hopsworks_aiomysql==0.2.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks_aiomysql[sa]==0.2.1->hopsworks==4.1.5) (0.2.1)\n",
      "Requirement already satisfied: opensearch-py<=2.4.2,>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (2.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (4.67.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (1.70.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.25.4 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from hopsworks==4.1.5) (4.25.6)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.1.5) (2.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.1.5) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.1.5) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks==4.1.5) (2024.12.14)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.2.0->hopsworks==4.1.5) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.2.0->hopsworks==4.1.5) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.2.0->hopsworks==4.1.5) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->hopsworks==4.1.5) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->hopsworks==4.1.5) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from sqlalchemy->hopsworks==4.1.5) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from sqlalchemy->hopsworks==4.1.5) (3.1.1)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.12 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from boto3->hopsworks==4.1.5) (1.36.12)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from boto3->hopsworks==4.1.5) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from boto3->hopsworks==4.1.5) (0.11.2)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from furl->hopsworks==4.1.5) (1.0.1)\n",
      "Requirement already satisfied: javaobj-py3 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyjks->hopsworks==4.1.5) (0.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyjks->hopsworks==4.1.5) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyjks->hopsworks==4.1.5) (0.4.1)\n",
      "Requirement already satisfied: pycryptodomex in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyjks->hopsworks==4.1.5) (3.21.0)\n",
      "Requirement already satisfied: twofish in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyjks->hopsworks==4.1.5) (0.3.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from PyMySQL[rsa]->hopsworks==4.1.5) (44.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->hopsworks==4.1.5) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from cryptography->PyMySQL[rsa]->hopsworks==4.1.5) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks==4.1.5) (2.22)\n"
     ]
    }
   ],
   "source": [
    "#!pip install hopsworks==4.1.5\n",
    "# !pip show hopsworks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32aea6ff-1de8-4c57-b3f0-19f2a634b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-07 06:33:35,049 INFO: Initializing external client\n",
      "2025-03-07 06:33:35,050 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-07 06:33:37,107 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215707\n",
      "Successfully connected to Hopsworks project: NYC_taxi_ammy\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "api_key = os.getenv('HOPSWORKS_API_KEY')  \n",
    "project_name = os.getenv('HOPSWORKS_PROJECT_NAME')  \n",
    "\n",
    "# pip install confluent-kafka\n",
    "# Initialize connection to Hopsworks  \n",
    "project = hopsworks.login(  \n",
    "    api_key_value=api_key,  \n",
    "    project=project_name  \n",
    ")  \n",
    "print(f\"Successfully connected to Hopsworks project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b6085d-dfff-47ef-9348-c74db450b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f175c6f-0685-47ae-a391-c8056ac817a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339cb11-957c-43e6-9f39-96cc675ecf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=FEATURE_GROUP_NAME,\n",
    "    version=FEATURE_GROUP_VERSION,\n",
    "    \n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be77e980-c3d2-4afb-bdf4-765ad255cfea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1215707/featurestores/1203333/featuregroups). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270121,\"usrMsg\":\"Stream enabled feature groups only support `HUDI` time travel format, which requires a primary key to be set.\",\"errorMsg\":\"Primary key is required when using Hudi time travel format\"}', error code: 270121, error msg: Primary key is required when using Hudi time travel format, user msg: Stream enabled feature groups only support `HUDI` time travel format, which requires a primary key to be set.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hsfs\\feature_group.py:2940\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[1;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait)\u001b[0m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2938\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[1;32m-> 2940\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2941\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2944\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m   2951\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[0;32m   2952\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[0;32m   2953\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:171\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[1;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options)\u001b[0m\n\u001b[0;32m    165\u001b[0m util\u001b[38;5;241m.\u001b[39mvalidate_embedding_feature_type(\n\u001b[0;32m    166\u001b[0m     feature_group\u001b[38;5;241m.\u001b[39membedding_index, dataframe_features\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_id:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# only save metadata if feature group does not exist\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_feature_group_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_schema_compatibility(\n\u001b[0;32m    177\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39mfeatures, dataframe_features\n\u001b[0;32m    178\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:482\u001b[0m, in \u001b[0;36mFeatureGroupEngine.save_feature_group_metadata\u001b[1;34m(self, feature_group, dataframe_features, write_options)\u001b[0m\n\u001b[0;32m    469\u001b[0m     _write_options \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    470\u001b[0m         [\n\u001b[0;32m    471\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: k, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: v}\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     )\n\u001b[0;32m    478\u001b[0m     feature_group\u001b[38;5;241m.\u001b[39m_deltastreamer_jobconf \u001b[38;5;241m=\u001b[39m DeltaStreamerJobConf(\n\u001b[0;32m    479\u001b[0m         _write_options, _spark_options\n\u001b[0;32m    480\u001b[0m     )\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Group created successfully, explore it at \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;241m+\u001b[39m util\u001b[38;5;241m.\u001b[39mget_feature_group_url(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     )\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hsfs\\core\\feature_group_api.py:60\u001b[0m, in \u001b[0;36mFeatureGroupApi.save\u001b[1;34m(self, feature_group_instance)\u001b[0m\n\u001b[0;32m     55\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectationsuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformationfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     57\u001b[0m }\n\u001b[0;32m     58\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     59\u001b[0m feature_group_object \u001b[38;5;241m=\u001b[39m feature_group_instance\u001b[38;5;241m.\u001b[39mupdate_from_response_json(\n\u001b[1;32m---> 60\u001b[0m     \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_group_object\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hopsworks_common\\decorators.py:45\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[1;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\myenv\\Lib\\site-packages\\hopsworks_common\\client\\base.py:186\u001b[0m, in \u001b[0;36mClient._send_request\u001b[1;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[0;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[0;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1215707/featurestores/1203333/featuregroups). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270121,\"usrMsg\":\"Stream enabled feature groups only support `HUDI` time travel format, which requires a primary key to be set.\",\"errorMsg\":\"Primary key is required when using Hudi time travel format\"}', error code: 270121, error msg: Primary key is required when using Hudi time travel format, user msg: Stream enabled feature groups only support `HUDI` time travel format, which requires a primary key to be set."
     ]
    }
   ],
   "source": [
    "feature_group.insert(ts_data, write_options = {\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223fcca-daf5-40f9-8cb3-5e5dd0c8887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac8c4d-c662-4fcb-87ac-c60b7fd0ec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 857.47 MB\n"
     ]
    }
   ],
   "source": [
    "df_memory_mb = rides.memory_usage(deep=True).sum() / (1024 * 1024)  \n",
    "print(f\"DataFrame size: {df_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ad25d-3d80-4b44-a119-46932bdd9570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
